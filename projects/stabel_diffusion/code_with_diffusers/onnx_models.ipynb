{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import onnx\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch.onnx import export\n",
    "\n",
    "from diffusers import OnnxRuntimeModel, OnnxStableDiffusionPipeline, StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_torch_less_than_1_11 = version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.11\")\n",
    "\n",
    "def onnx_export(\n",
    "    model,\n",
    "    model_args: tuple,\n",
    "    output_path: Path,\n",
    "    ordered_input_names,\n",
    "    output_names,\n",
    "    dynamic_axes,\n",
    "    opset,\n",
    "    use_external_data_format=False,\n",
    "):\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # PyTorch deprecated the `enable_onnx_checker` and `use_external_data_format` arguments in v1.11,\n",
    "    # so we check the torch version for backwards compatibility\n",
    "    if is_torch_less_than_1_11:\n",
    "        export(\n",
    "            model,\n",
    "            model_args,\n",
    "            f=output_path.as_posix(),\n",
    "            input_names=ordered_input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "            do_constant_folding=True,\n",
    "            use_external_data_format=use_external_data_format,\n",
    "            enable_onnx_checker=True,\n",
    "            opset_version=opset,\n",
    "        )\n",
    "    else:\n",
    "        export(\n",
    "            model,\n",
    "            model_args,\n",
    "            f=output_path.as_posix(),\n",
    "            input_names=ordered_input_names,\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "            do_constant_folding=True,\n",
    "            opset_version=opset,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767eb462054a4f91b7f30cada81ebf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9a0215e1024160ac4a1a7beee81936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7624ae301a52401f93efbe20ed6852b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a464753c9a814633a5ba7445d1f66c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518b1a3a3599484aa8a688a7ed814b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19021bb19f34aa7a8f5ee2ea2b41c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61a3003cc804819b9cfe23d1b463695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca71f4c7538460c8f1eb370689da828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097520e5290a4dbb8a65c236f4aafd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5cc0c68b1c4c739344b47bbf864fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4548d23a007b4990a4f441deaf5b5152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05928d75fa542e8a901cdbfeb37bdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815b906d8111483fbc91c51770c2de2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b356cdb0db744d75ab7076ddd81ae0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586fbdee273341d0851f82809db1acf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b40a21983d48929dc49c8494cbded2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path  = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "output_path = \"./onnx_models_diffusers\"\n",
    "opset       = 14\n",
    "fp16        = False\n",
    "\n",
    "dtype = torch.float16 if fp16 else torch.float32\n",
    "if fp16 and torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif fp16 and not torch.cuda.is_available():\n",
    "    raise ValueError(\"`float16` model export is only supported on GPUs with CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=dtype).to(device)\n",
    "output_path = Path(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:281: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:289: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:321: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5408: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TEXT ENCODER\n",
    "num_tokens = pipeline.text_encoder.config.max_position_embeddings\n",
    "text_hidden_size = pipeline.text_encoder.config.hidden_size\n",
    "text_input = pipeline.tokenizer(\n",
    "    \"A sample prompt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipeline.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "onnx_export(\n",
    "    pipeline.text_encoder,\n",
    "    # casting to torch.int32 until the CLIP fix is released: https://github.com/huggingface/transformers/pull/18515/files\n",
    "    model_args=(text_input.input_ids.to(device=device, dtype=torch.int32)),\n",
    "    output_path=output_path / \"text_encoder\" / \"model.onnx\",\n",
    "    ordered_input_names=[\"input_ids\"],\n",
    "    output_names=[\"last_hidden_state\", \"pooler_output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n",
    "    },\n",
    "    opset=opset,\n",
    ")\n",
    "del pipeline.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:266: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not return_dict:\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# VAE DECODER\n",
    "vae_decoder = pipeline.vae\n",
    "vae_latent_channels = vae_decoder.config.latent_channels\n",
    "vae_out_channels = vae_decoder.config.out_channels\n",
    "    # forward only through the decoder part\n",
    "vae_encoder = pipeline.vae\n",
    "vae_decoder.forward = vae_encoder.decode\n",
    "onnx_export(\n",
    "        vae_decoder,\n",
    "        model_args=(\n",
    "            torch.randn(1, vae_latent_channels, 64, 64).to(device=device, dtype=dtype),\n",
    "            False,\n",
    "        ),\n",
    "        output_path=output_path / \"vae_decoder\" / \"model.onnx\",\n",
    "        ordered_input_names=[\"latent_sample\", \"return_dict\"],\n",
    "        output_names=[\"sample\"],\n",
    "        dynamic_axes={\n",
    "            \"latent_sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n",
    "        },\n",
    "        opset=opset,\n",
    ")\n",
    "del pipeline.vae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
