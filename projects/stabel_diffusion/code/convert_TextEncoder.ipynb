{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.schedulers import PNDMScheduler\n",
    "from pathlib import Path\n",
    "from diffusers import DiffusionPipeline\n",
    "from util import onnx_export\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, tokenizer, textencoder, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = textencoder.to(device = device)\n",
    "        self.device = device\n",
    "       \n",
    "    def forward(self, text_ids):\n",
    "        # uncond-input ì¤€ë¹„\n",
    "        uncond_input = self.tokenizer(\n",
    "            [\"\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=pipeline.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "        # ì¸ì½”ë”© \n",
    "        textembed               = self.text_encoder(text_ids.to(device=self.device, dtype=torch.int32)).last_hidden_state\n",
    "        negative_prompt_embeds  = self.text_encoder(uncond_input.to(device=self.device, dtype=torch.int32)).last_hidden_state\n",
    "        \n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, textembed])\n",
    "        return prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:721: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return torch.tensor(value)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:281: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:289: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:321: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export FinishðŸ·\n"
     ]
    }
   ],
   "source": [
    "text_input = pipeline.tokenizer(\n",
    "    [\"A smile Tiger\"],\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipeline.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    ).input_ids.to(device=device, dtype=torch.int32)\n",
    "\n",
    "onnx_export(\n",
    "    TextEmbedding(pipeline.tokenizer, pipeline.text_encoder, device),\n",
    "    model_args=(\n",
    "        text_input\n",
    "    ),\n",
    "    output_path = Path('../onnx-models/TextEmbedding.onnx'),\n",
    "    ordered_input_names=[\"prompt\"],\n",
    "    output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "    dynamic_axes={\n",
    "        \"prompt\": {0: \"batch\", 1: \"sequence\"},\n",
    "    },\n",
    "    opset=14,\n",
    "    use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
