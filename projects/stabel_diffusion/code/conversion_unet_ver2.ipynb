{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.schedulers import PNDMScheduler\n",
    "from pathlib import Path\n",
    "from diffusers import DiffusionPipeline\n",
    "from util import onnx_export\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "from onnxruntime.quantization import QuantType\n",
    "import onnx \n",
    "from diffusers.utils import BaseOutput\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (1) Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet2DConditionModel_Down(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.num_upsamplers = unet.num_upsamplers\n",
    "        self.time_proj = unet.time_proj\n",
    "        self.time_embedding = unet.time_embedding\n",
    "        self.conv_in = unet.conv_in\n",
    "        self.down_blocks = unet.down_blocks\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "    ):\n",
    "                \n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        \n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "        \n",
    "        t_emb = self.time_proj(timesteps)\n",
    "        \n",
    "        emb = self.time_embedding(t_emb, None)\n",
    "        aug_emb = None\n",
    "        emb = emb + aug_emb if aug_emb is not None else emb\n",
    "        \n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)   # 2 320 64 64\n",
    "        print(sample.shape)\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=None,\n",
    "                    cross_attention_kwargs=None,\n",
    "                    encoder_attention_mask=None,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        return sample, emb, down_block_res_samples[0], down_block_res_samples[1],down_block_res_samples[2],\\\n",
    "                down_block_res_samples[3], down_block_res_samples[4], down_block_res_samples[5], \\\n",
    "                down_block_res_samples[6], down_block_res_samples[7], down_block_res_samples[8], \\\n",
    "                down_block_res_samples[9], down_block_res_samples[10], down_block_res_samples[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export Start🚗\n",
      "torch.Size([2, 320, 64, 64])\n",
      "ONNX export Finish🍷\n"
     ]
    }
   ],
   "source": [
    "onnx_export(\n",
    "        UNet2DConditionModel_Down(pipeline.unet),\n",
    "        model_args=(\n",
    "            torch.randn([2,4,64,64]).to(device=device, dtype=dtype),\n",
    "            torch.randn(1).to(device=device, dtype=dtype),\n",
    "            torch.randn([2,77,768]).to(device=device, dtype=dtype),\n",
    "        ),\n",
    "        output_path = Path('../onnx-models/UNet-ver2/down.onnx'),\n",
    "        ordered_input_names=[\"sample\", \"timestep\", \"encoder_hidden_states\"],\n",
    "        output_names=[\n",
    "            \"out_sample\", \"emb\",\n",
    "            'down_block_res_samples_0', 'down_block_res_samples_1', 'down_block_res_samples_2',\n",
    "            'down_block_res_samples_3', 'down_block_res_samples_4', 'down_block_res_samples_5',\n",
    "            'down_block_res_samples_6', 'down_block_res_samples_7', 'down_block_res_samples_8',\n",
    "            'down_block_res_samples_9', 'down_block_res_samples_10', 'down_block_res_samples_11',\n",
    "        ],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"timestep\": {0: \"batch\"}\n",
    "            # \"sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n",
    "            # \"timestep\": {0: \"batch\"},\n",
    "            # \"encoder_hidden_states\": {0: \"batch\", 1: \"sequence\"},\n",
    "        },\n",
    "        opset=14,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961M\t../onnx-models/UNet-ver2/down.onnx\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../onnx-models/UNet-ver2/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (2) Mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Mid(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.mid_block = unet.mid_block\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        emb: torch.FloatTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "    ):\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=None,\n",
    "                cross_attention_kwargs=None,\n",
    "                encoder_attention_mask=None,\n",
    "            )\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export Start🚗\n",
      "ONNX export Finish🍷\n"
     ]
    }
   ],
   "source": [
    "onnx_export(\n",
    "        UNet2DConditionModel_Mid(pipeline.unet),\n",
    "        model_args=(\n",
    "            torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype),\n",
    "            torch.randn(2, 1280).to(device=device, dtype=dtype),\n",
    "            torch.randn([2, 77, 768]).to(device=device, dtype=dtype),\n",
    "        ),\n",
    "        output_path = Path('../onnx-models/UNet-ver2/unet2dconditionalmodel_mid.onnx'),\n",
    "        ordered_input_names=[\"sample\", \"emb\", \"encoder_hidden_states\"],\n",
    "        output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"emb\": {1: \"sequence\"}\n",
    "            # \"sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n",
    "            # \"timestep\": {0: \"batch\"},\n",
    "            # \"encoder_hidden_states\": {0: \"batch\", 1: \"sequence\"},\n",
    "        },\n",
    "        opset=14,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961M\t../onnx-models/UNet-ver2/down.onnx\n",
      "371M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_mid.onnx\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../onnx-models/UNet-ver2/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Up(nn.Module):\n",
    "    def __init__(self, unet_up_ith_block):\n",
    "        super().__init__()\n",
    "        self.upsample_block = unet_up_ith_block\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        emb: torch.FloatTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        down_block_res_samples0,\n",
    "        down_block_res_samples1,\n",
    "        down_block_res_samples2,\n",
    "    ):\n",
    "        res_samples = [\n",
    "            down_block_res_samples0, down_block_res_samples1, down_block_res_samples2\n",
    "        ]\n",
    "        # print(sample.shape)\n",
    "        if hasattr(self.upsample_block, \"has_cross_attention\") and self.upsample_block.has_cross_attention:\n",
    "            sample = self.upsample_block(\n",
    "                hidden_states=sample,\n",
    "                temb=emb,\n",
    "                res_hidden_states_tuple=res_samples,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                cross_attention_kwargs=None,\n",
    "                upsample_size=None,\n",
    "                attention_mask=None,\n",
    "                encoder_attention_mask=None,\n",
    "            )\n",
    "        else:\n",
    "            sample = self.upsample_block(\n",
    "                hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=None\n",
    "            )\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, tokenizer, textencoder, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = textencoder.to(device = device)\n",
    "        self.device = device\n",
    "       \n",
    "    def forward(self, text_ids):\n",
    "        # uncond-input 준비\n",
    "        uncond_input = self.tokenizer(\n",
    "            [\"\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=pipeline.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "        # 인코딩 \n",
    "        textembed               = self.text_encoder(text_ids.to(device=self.device, dtype=torch.int32)).last_hidden_state\n",
    "        negative_prompt_embeds  = self.text_encoder(uncond_input.to(device=self.device, dtype=torch.int32)).last_hidden_state\n",
    "        \n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, textembed])\n",
    "        return prompt_embeds\n",
    "text_input = pipeline.tokenizer(\n",
    "    [\"A smile Tiger\"],\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipeline.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    ).input_ids.to(device=device, dtype=torch.int32)\n",
    "te = TextEmbedding(pipeline.tokenizer, pipeline.text_encoder, device)\n",
    "text_embed = te(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export Start🚗\n",
      "ONNX export Finish🍷\n",
      "ONNX export Start🚗\n",
      "ONNX export Finish🍷\n",
      "ONNX export Start🚗\n",
      "ONNX export Finish🍷\n"
     ]
    }
   ],
   "source": [
    "down_block_res_samples = [\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    \n",
    "    torch.randn([2, 320, 32, 32]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 640, 32, 32]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 640, 32, 32]).to(device=device, dtype=dtype),\n",
    "    \n",
    "    torch.randn([2, 640, 16, 16]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 16, 16]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 16, 16]).to(device=device, dtype=dtype),\n",
    "            \n",
    "    torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype)   \n",
    "]\n",
    "\n",
    "pipeline.unet.ir_version = 11\n",
    "for i in range(4):\n",
    "    res_samples = down_block_res_samples[-3 :]\n",
    "    down_block_res_samples = down_block_res_samples[: -3]\n",
    "    if i == 3: continue\n",
    "    onnx_export(\n",
    "            UNet2DConditionModel_Up(pipeline.unet.up_blocks[i]),\n",
    "            model_args=(\n",
    "                torch.randn([2, 1280 if i!=3 else 640, 8*(2**(i)), 8*(2**(i))]).to(device=device, dtype=dtype),\n",
    "                torch.randn([2, 1280]).to(device=device, dtype=dtype),\n",
    "                text_embed, #torch.randn([2, 77, 768]).to(device=device, dtype=dtype),\n",
    "                res_samples[0], res_samples[1], res_samples[2]\n",
    "            ),\n",
    "            output_path = Path(f'../onnx-models/UNet-ver2/unet2dconditionalmodel_up_{i}/model.onnx'),\n",
    "            ordered_input_names=[\n",
    "                \"sample\", \"emb\", \"encoder_hidden_states\", \n",
    "                \"res_samples0\", \"res_samples1\", \"res_samples2\",\n",
    "            ],\n",
    "            output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "            dynamic_axes={\n",
    "                \"sample\": {0:\"b\"},\n",
    "                \"emb\": {1: \"sequence\"},\n",
    "                \n",
    "            },\n",
    "            opset=11,\n",
    "            use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "        )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Up_3(nn.Module):\n",
    "    def __init__(self, resnets, attentions):\n",
    "        super().__init__()\n",
    "        self.resnet = resnets\n",
    "        self.attention = attentions\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        emb: torch.FloatTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        res_hidden_states,\n",
    "    ):\n",
    "        sample = torch.cat([sample, res_hidden_states], dim=1)\n",
    "        sample = self.resnet(sample, emb)\n",
    "        sample = self.attention(\n",
    "            sample,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            cross_attention_kwargs=None,\n",
    "            attention_mask=None,\n",
    "            encoder_attention_mask=None,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "        print(sample.shape)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export Start🚗\n",
      "torch.Size([2, 320, 64, 64])\n",
      "ONNX export Finish🍷\n",
      "ONNX export Start🚗\n",
      "torch.Size([2, 320, 64, 64])\n",
      "ONNX export Finish🍷\n",
      "ONNX export Start🚗\n",
      "torch.Size([2, 320, 64, 64])\n",
      "ONNX export Finish🍷\n"
     ]
    }
   ],
   "source": [
    "res_samples = [          \n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "]\n",
    "pipeline.unet.ir_version = 11\n",
    "for i in range(3):\n",
    "    onnx_export(\n",
    "        UNet2DConditionModel_Up_3(\n",
    "            pipeline.unet.up_blocks[-1].resnets[i],\n",
    "            pipeline.unet.up_blocks[-1].attentions[i]\n",
    "        ),\n",
    "        model_args=(\n",
    "            torch.randn([2, 640 if i == 0 else 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "            torch.randn([2, 1280]).to(device=device, dtype=dtype),\n",
    "            text_embed,\n",
    "            res_samples[-i], \n",
    "        ),\n",
    "        output_path = Path(f'../onnx-models/UNet-ver2/unet2dconditionalmodel_up_{3}_{i}/model.onnx'),\n",
    "        ordered_input_names=[\n",
    "            \"sample\", \"emb\", \"encoder_hidden_states\", \n",
    "            \"res_samples0\"\n",
    "        ],\n",
    "        output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"sample\": {0:\"b\"},        \n",
    "        },\n",
    "        opset=11,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961M\t../onnx-models/UNet-ver2/down.onnx\n",
      "371M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_mid.onnx\n",
      "619M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_0\n",
      "1002M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_1\n",
      "480M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_2\n",
      "1.1G\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_3_0\n",
      "1.1G\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_3_1\n",
      "1.1G\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_3_2\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../onnx-models/UNet-ver2/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (4) post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_post_process(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.conv_norm_out  = unet.conv_norm_out\n",
    "        self.conv_act       = unet.conv_act\n",
    "        self.conv_out       = unet.conv_out\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor\n",
    "    ):\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export Start🚗\n",
      "ONNX export Finish🍷\n",
      "56K\t../onnx-models/UNet-ver2/UNet_post_process\n",
      "961M\t../onnx-models/UNet-ver2/down.onnx\n",
      "371M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_mid.onnx\n",
      "619M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_0\n",
      "1002M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_1\n",
      "480M\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_2\n",
      "1.1G\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_3_0\n",
      "1.1G\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_3_1\n",
      "1.1G\t../onnx-models/UNet-ver2/unet2dconditionalmodel_up_3_2\n"
     ]
    }
   ],
   "source": [
    "onnx_export(\n",
    "        UNet_post_process(pipeline.unet),\n",
    "        model_args=(\n",
    "            torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "        ),\n",
    "        output_path = Path('../onnx-models/UNet-ver2/UNet_post_process/model.onnx'),\n",
    "        ordered_input_names=[\"sample\"],\n",
    "        output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"sample\": {0: \"batch\"}\n",
    "        },\n",
    "        opset=14,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )\n",
    "!du -sh ../onnx-models/UNet-ver2/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_onnx_model(input_path, output_path):\n",
    "    print(f\"start quantize : {input_path} => {output_path}\")\n",
    "    quantize_dynamic(\n",
    "        model_input     = input_path,\n",
    "        model_output    = output_path,\n",
    "        per_channel     = False,\n",
    "        reduce_range    = False,\n",
    "        weight_type     = QuantType.QUInt8,\n",
    "    )\n",
    "    print(f\"end : {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
