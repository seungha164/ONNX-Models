{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from simple_tokenizer import SimpleTokenizer as _Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "save_path = f'../onnx_models_{device}'\n",
    "os.makedirs(save_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 97,  32, 115, 109, 105, 108, 101,  32,  99,  97, 116])\n",
      "tensor([[ 97,  32, 115, 109, 105, 108, 101,  32,  99,  97, 116,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
      "          -1,  -1]])\n"
     ]
    }
   ],
   "source": [
    "# 1. ì•„ìŠ¤í‚¤ ì½”ë“œí™”\n",
    "string = \"a smile cat\"\n",
    "ascii_codes = np.array([ord(char) for char in string])\n",
    "ascii_codes = torch.tensor(ascii_codes)\n",
    "print(ascii_codes)  # ì¶œë ¥: [72 101 108 108 111]\n",
    "\n",
    "# 2. ë‚˜ë¨¸ì§€ '-1'ë¡œ ì±„ì›Œì„œ, [1,100] ì‚¬ì´ì¦ˆì˜ tensorí™”\n",
    "arr = torch.full((1, 100), -1).to(device = device)\n",
    "arr[0, :ascii_codes.size(0)] = ascii_codes\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### ğŸ’› Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3571847/3836549866.py:10: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  re = self.tokenizer.encode(''.join([chr(_i) for _i in x]))\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Inference failed or unsupported type to quantize for tensor '/NonZero_output_0', type is tensor_type {\n",
      "  elem_type: 7\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_value: 2\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"unk__0\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "class TokenizerModule(nn.Module):\n",
    "    def __init__(self, _tokenizer, device):\n",
    "        super().__init__()\n",
    "        self.tokenizer = _tokenizer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[x != -1]\n",
    "        llen = 77 - x.shape[0]\n",
    "        result = torch.zeros(1, x.shape[0] + llen, dtype=torch.int64)\n",
    "        re = self.tokenizer.encode(''.join([chr(_i) for _i in x]))\n",
    "        result[0][0] = 49407\n",
    "        for i in range(len(re)):\n",
    "            result[0][i+1] = re[i]\n",
    "        result[0][i+2] = 49406\n",
    "        return result\n",
    "\n",
    "\n",
    " # onnx conversion\n",
    "os.makedirs(f'{save_path}/tokenizer/', exist_ok= True)\n",
    "torch.onnx.export(\n",
    "    model               =   TokenizerModule(_tokenizer, device),                            # ì‹¤í–‰ë  ëª¨ë¸\n",
    "    args                =   (arr),        # ëª¨ë¸ ì…ë ¥ê°’(tuple or ì—¬ëŸ¬ ì…ë ¥ê°’)\n",
    "    f                   =   f'{save_path}/tokenizer/to_origin.onnx',                     # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "    export_params       =   True,                 # ëª¨ë¸ íŒŒì¼ ì•ˆì— í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì—¬ë¶€\n",
    "    opset_version       =   14,                   # ëª¨ë¸ ë³€í™˜í•  ë•Œ ì‚¬ìš©í•  onnx ë²„ì „\n",
    "    do_constant_folding =   True,         # ìµœì í™”ì‹œ ìƒìˆ˜í´ë”© ì‚¬ìš©í• ì§€ ì—¬ë¶€\n",
    "    input_names     =   ['input'],\n",
    "    output_names    =   [\"output\"],\n",
    "    dynamic_axes    =   {\n",
    "        'input'     : {0 : 'batch_size'},    # ê°€ë³€ì ì¸ ê¸¸ì´ë¥¼ ê°€ì§„ ì°¨ì›\n",
    "    }\n",
    ") \n",
    "# model quantization\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'./{save_path}/tokenizer/to_origin.onnx', \n",
    "    model_output    =   f'./{save_path}/tokenizer/to_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ğŸ’š ONNX-Runtime Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names: ['input']\n",
      "Output names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = f'{save_path}/tokenizer/to_quant.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Print the input names and shapes\n",
    "input_names = [input.name for input in session.get_inputs()]\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "print(\"Input names:\", input_names)\n",
    "print(\"Output names:\", output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 77)\n",
      "[[49407   320  3490  2368 49406     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# test running\n",
    "ort_inputs  = {'input': arr.detach().cpu().numpy()}\n",
    "ort_outputs = session.run(None, ort_inputs)\n",
    "print(ort_outputs[0].shape)\n",
    "print(ort_outputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
