{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.schedulers import PNDMScheduler\n",
    "from pathlib import Path\n",
    "from diffusers import DiffusionPipeline\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "from onnxruntime.quantization import QuantType\n",
    "\n",
    "from util_onnx import onnx_export\n",
    "import utils\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "dtype = torch.float32\n",
    "save_path = '../onnx_models_cuda'\n",
    "os.makedirs(save_path, exist_ok = True)\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 0. INPUT ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 77, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = f'{save_path}/tokenizer/to_quant.onnx'\n",
    "sessTokenizer = ort.InferenceSession(onnx_model_path, providers=['AzureExecutionProvider'])\n",
    "onnx_model_path = f'{save_path}/text_encoder/te_quant.onnx'\n",
    "sessionTextEncoder = ort.InferenceSession(onnx_model_path, providers=['AzureExecutionProvider'])\n",
    "\n",
    "ascii_str   = utils.toAsciiTensor()\n",
    "text_ids = sessTokenizer.run(None, {\n",
    "    'input' : ascii_str.detach().cpu().numpy()\n",
    "})[0]\n",
    "ort_output = sessionTextEncoder.run(None, {\n",
    "    'input' : text_ids\n",
    "})[0]\n",
    "ort_output.shape\n",
    "# text_ids = torch.tensor(input).to(device = device)\n",
    "# text_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "##### 1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PNDMScheduler.scale_model_input of PNDMScheduler {\n",
       "  \"_class_name\": \"PNDMScheduler\",\n",
       "  \"_diffusers_version\": \"0.18.0\",\n",
       "  \"beta_end\": 0.012,\n",
       "  \"beta_schedule\": \"scaled_linear\",\n",
       "  \"beta_start\": 0.00085,\n",
       "  \"clip_sample\": false,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"set_alpha_to_one\": false,\n",
       "  \"skip_prk_steps\": true,\n",
       "  \"steps_offset\": 1,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null\n",
       "}\n",
       ">"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.scheduler.scale_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    }
   ],
   "source": [
    "class UnetModel_pre(nn.Module):\n",
    "    def __init__(self, scheduler):\n",
    "        super().__init__()\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def forward(self, latents):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        return torch.concat([latents] * 2)\n",
    "        \n",
    "onnx_export(\n",
    "    UnetModel_pre(pipeline.scheduler),\n",
    "    model_args=(\n",
    "        torch.randn([1, 4, 64, 64]).to(device = device, dtype = dtype),\n",
    "        # torch.tensor([981]).to(device = device, dtype = dtype),\n",
    "    ),\n",
    "    output_path = Path(f'{save_path}/unet/upre_origin.onnx'),\n",
    "    ordered_input_names=[\n",
    "        \"latents\", \n",
    "        # 'timestep'\n",
    "    ],\n",
    "    output_names=[\"output\"],  # has to be different from \"sample\" for correct tracing\n",
    "    dynamic_axes={ \n",
    "        \"latents\": {0: \"batch\"},\n",
    "        # \"timestep\": {0: \"batch\"},\n",
    "    },\n",
    "    opset=12,\n",
    "    use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    ")\n",
    "# model quantization\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'{save_path}/unet/upre_origin.onnx', \n",
    "    model_output    =   f'{save_path}/unet/upre_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    }
   ],
   "source": [
    "class UNet_post(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor                   # [2, 320, 64, 64]\n",
    "    ):\n",
    "        # perform guidance -guidance_scale : 7.5\n",
    "        noise_pred_uncond, noise_pred_text = sample.chunk(2)\n",
    "        sample = noise_pred_uncond + 7.5 * (noise_pred_text - noise_pred_uncond)\n",
    "        return sample\n",
    "\n",
    "onnx_export(\n",
    "    UNet_post(),\n",
    "    model_args=(\n",
    "        torch.randn([2, 4, 64, 64]).to(device = device, dtype = dtype),\n",
    "    ),\n",
    "    output_path = Path(f'{save_path}/unet/upost_final_origin.onnx'),\n",
    "    ordered_input_names=[\n",
    "        \"sample\", \n",
    "        # 'timestep'\n",
    "    ],\n",
    "    output_names=[\"output\"],  # has to be different from \"sample\" for correct tracing\n",
    "    dynamic_axes={ \n",
    "        \"sample\": {0: \"batch\"},\n",
    "        # \"timestep\": {0: \"batch\"},\n",
    "    },\n",
    "    opset=12,\n",
    "    use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    ")\n",
    "# model quantization\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'{save_path}/unet/upost_final_origin.onnx', \n",
    "    model_output    =   f'{save_path}/unet/upost_final_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_post_process(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.conv_norm_out  = unet.conv_norm_out.to(device = device)    # (32, 320)\n",
    "        self.conv_act       = unet.conv_act.to(device = device)         # SiLU()\n",
    "        self.conv_out       = unet.conv_out.to(device = device)         # (320, 4)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor                   # [2, 320, 64, 64]\n",
    "    ):\n",
    "        #* 6. post-process\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "        # perform guidance -guidance_scale : 7.5\n",
    "        noise_pred_uncond, noise_pred_text = sample.chunk(2)\n",
    "        sample = noise_pred_uncond + 7.5 * (noise_pred_text - noise_pred_uncond)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Down(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.num_upsamplers = unet.num_upsamplers\n",
    "        self.time_proj      = unet.time_proj.to(device = device)\n",
    "        self.time_embedding = unet.time_embedding.to(device = device)\n",
    "        self.conv_in        = unet.conv_in.to(device = device)\n",
    "        self.down_blocks    = unet.down_blocks.to(device = device)\n",
    "        self.mid_block      = unet.mid_block.to(device = device)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "    ):\n",
    "\n",
    "        #* 1. time -  tensor([981])\n",
    "        timestep = timestep.expand(sample.shape[0])\n",
    "        t_emb = self.time_proj(timestep).to(dtype=sample.dtype)\n",
    "        emb = self.time_embedding(t_emb, None)\n",
    "        \n",
    "        #* 2. pre-process\n",
    "        sample = self.conv_in(sample).to(device = sample.device)   # 2 320 64 64\n",
    "        \n",
    "        #* 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=None,\n",
    "                    cross_attention_kwargs=None,\n",
    "                    encoder_attention_mask=None,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "            down_block_res_samples += res_samples\n",
    "        \n",
    "        \n",
    "        return sample, emb, \\\n",
    "            down_block_res_samples[0], down_block_res_samples[1],down_block_res_samples[2],\\\n",
    "            down_block_res_samples[3], down_block_res_samples[4], down_block_res_samples[5], \\\n",
    "            down_block_res_samples[6], down_block_res_samples[7], down_block_res_samples[8], \\\n",
    "            down_block_res_samples[9], down_block_res_samples[10], down_block_res_samples[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Mid(nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.mid_block = unet.mid_block\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        emb: torch.FloatTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "    ):\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=None,\n",
    "                cross_attention_kwargs=None,\n",
    "                encoder_attention_mask=None,\n",
    "            )\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Up(nn.Module):\n",
    "    def __init__(self, unet_up_ith_block):\n",
    "        super().__init__()\n",
    "        self.upsample_block = unet_up_ith_block.to(device = device)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        emb: torch.FloatTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        down_block_res_samples0,\n",
    "        down_block_res_samples1,\n",
    "        down_block_res_samples2,\n",
    "    ):\n",
    "        res_samples = [\n",
    "            down_block_res_samples0, down_block_res_samples1, down_block_res_samples2\n",
    "        ]\n",
    "        if hasattr(self.upsample_block, \"has_cross_attention\") and self.upsample_block.has_cross_attention:\n",
    "            sample = self.upsample_block(\n",
    "                hidden_states=sample,\n",
    "                temb=emb,\n",
    "                res_hidden_states_tuple=res_samples,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                cross_attention_kwargs=None,\n",
    "                upsample_size=None,\n",
    "                attention_mask=None,\n",
    "                encoder_attention_mask=None,\n",
    "            )\n",
    "        else:\n",
    "            sample = self.upsample_block(\n",
    "                hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=None\n",
    "            )\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet2DConditionModel_Up_3(nn.Module):\n",
    "    def __init__(self, resnets, attentions):\n",
    "        super().__init__()\n",
    "        self.resnet = resnets.to(device = device)\n",
    "        self.attention = attentions.to(device = device)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        emb: torch.FloatTensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        res_hidden_states,\n",
    "    ):\n",
    "        sample = torch.cat([sample, res_hidden_states], dim=1)\n",
    "        sample = self.resnet(sample, emb)\n",
    "        sample = self.attention(\n",
    "            sample,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            cross_attention_kwargs=None,\n",
    "            attention_mask=None,\n",
    "            encoder_attention_mask=None,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "        # print(sample.shape)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### ðŸ’› Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export FinishðŸ·\n"
     ]
    }
   ],
   "source": [
    "\n",
    "onnx_export(\n",
    "    UNet_post_process(pipeline.unet),\n",
    "    model_args=(\n",
    "        torch.randn([2, 320, 64, 64]).to(device = device)\n",
    "    ),\n",
    "    output_path = Path(f'{save_path}/unet/upost_origin.onnx'),\n",
    "    ordered_input_names=[\"input\"],\n",
    "    output_names=[\"output\"],  # has to be different from \"sample\" for correct tracing\n",
    "    dynamic_axes={ \n",
    "        \"input\": {0: \"batch\"}\n",
    "    },\n",
    "    opset=12,\n",
    "    use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'{save_path}/unet/upost_origin.onnx', \n",
    "    model_output    =   f'{save_path}/unet/upost_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "onnx_export(\n",
    "        UNet2DConditionModel_Down(pipeline.unet),\n",
    "        model_args=(\n",
    "            torch.randn([2, 4, 64, 64]).to(device=device, dtype=dtype),\n",
    "            torch.randn([1]).to(device=device, dtype=dtype),\n",
    "            torch.randn([2, 77, 768]).to(device=device, dtype=dtype),\n",
    "        ),\n",
    "        output_path = Path(f'{save_path}/unet/udown_origin.onnx'),\n",
    "        ordered_input_names=[\"sample\", \"timestep\", \"encoder_hidden_states\"],\n",
    "        output_names=[\n",
    "            \"out_sample\", \"emb\",\n",
    "            'down_block_res_samples_0', 'down_block_res_samples_1', 'down_block_res_samples_2',\n",
    "            'down_block_res_samples_3', 'down_block_res_samples_4', 'down_block_res_samples_5',\n",
    "            'down_block_res_samples_6', 'down_block_res_samples_7', 'down_block_res_samples_8',\n",
    "            'down_block_res_samples_9', 'down_block_res_samples_10', 'down_block_res_samples_11',\n",
    "        ],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"timestep\": {0: \"batch\"}\n",
    "            # \"sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n",
    "            # \"timestep\": {0: \"batch\"},\n",
    "            # \"encoder_hidden_states\": {0: \"batch\", 1: \"sequence\"},\n",
    "        },\n",
    "        opset=14,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'{save_path}/unet/udown_origin.onnx', \n",
    "    model_output    =   f'{save_path}/unet/udown_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:2036: UserWarning: Provided key timestep for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "onnx_export(\n",
    "        UNet2DConditionModel_Mid(pipeline.unet),\n",
    "        model_args=(\n",
    "            torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype),\n",
    "            torch.randn(2, 1280).to(device=device, dtype=dtype),\n",
    "            torch.randn([2, 77, 768]).to(device=device, dtype=dtype),\n",
    "        ),\n",
    "        output_path = Path(f'{save_path}/unet/umid_origin.onnx'),\n",
    "        ordered_input_names=[\"sample\", \"emb\", \"encoder_hidden_states\"],\n",
    "        output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"timestep\": {0: \"batch\"}\n",
    "            # \"sample\": {0: \"batch\", 1: \"channels\", 2: \"height\", 3: \"width\"},\n",
    "            # \"timestep\": {0: \"batch\"},\n",
    "            # \"encoder_hidden_states\": {0: \"batch\", 1: \"sequence\"},\n",
    "        },\n",
    "        opset=14,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'{save_path}/unet/umid_origin.onnx', \n",
    "    model_output    =   f'{save_path}/unet/umid_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "down_block_res_samples = [\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    \n",
    "    torch.randn([2, 320, 32, 32]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 640, 32, 32]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 640, 32, 32]).to(device=device, dtype=dtype),\n",
    "    \n",
    "    torch.randn([2, 640, 16, 16]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 16, 16]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 16, 16]).to(device=device, dtype=dtype),\n",
    "            \n",
    "    torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 1280, 8, 8]).to(device=device, dtype=dtype)   \n",
    "]\n",
    "\n",
    "pipeline.unet.ir_version = 11\n",
    "for i in range(4):\n",
    "    res_samples = down_block_res_samples[-3 :]\n",
    "    down_block_res_samples = down_block_res_samples[: -3]\n",
    "    if i == 3: continue\n",
    "    onnx_export(\n",
    "            UNet2DConditionModel_Up(pipeline.unet.up_blocks[i]),\n",
    "            model_args=(\n",
    "                torch.randn([2, 1280 if i!=3 else 640, 8*(2**(i)), 8*(2**(i))]).to(device=device, dtype=dtype),\n",
    "                torch.randn([2, 1280]).to(device=device, dtype=dtype),\n",
    "                torch.tensor(ort_output).to(device=device, dtype=dtype), #torch.randn([2, 77, 768]).to(device=device, dtype=dtype),\n",
    "                res_samples[0], res_samples[1], res_samples[2]\n",
    "            ), \n",
    "            output_path = Path(f'{save_path}/unet/uup-{i}_origin.onnx'),\n",
    "            ordered_input_names=[\n",
    "                \"sample\", \"emb\", \"encoder_hidden_states\", \n",
    "                \"res_samples0\", \"res_samples1\", \"res_samples2\",\n",
    "            ],\n",
    "            output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "            dynamic_axes={\n",
    "                \"sample\": {0:\"b\"},\n",
    "                \"emb\": {1: \"sequence\"},\n",
    "                \n",
    "            },\n",
    "            opset=11,\n",
    "            use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )\n",
    "    quantize_dynamic(\n",
    "        model_input     =   f'{save_path}/unet/uup-{i}_origin.onnx', \n",
    "        model_output    =   f'{save_path}/unet/uup-{i}_quant.onnx', \n",
    "        per_channel     =   False,\n",
    "        reduce_range    =   False,\n",
    "        weight_type     =   QuantType.QUInt8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export StartðŸš—\n",
      "ONNX export FinishðŸ·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "res_hidden_states_tuple = [          \n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "    torch.randn([2, 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "]\n",
    "pipeline.unet.ir_version = 11\n",
    "for i in range(3):\n",
    "    # pop res hidden states\n",
    "    res_hidden_states = res_hidden_states_tuple[-1]\n",
    "    res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n",
    "    onnx_export(\n",
    "        UNet2DConditionModel_Up_3(\n",
    "            pipeline.unet.up_blocks[-1].resnets[i],\n",
    "            pipeline.unet.up_blocks[-1].attentions[i]\n",
    "        ),\n",
    "        model_args=(\n",
    "            torch.randn([2, 640 if i == 0 else 320, 64, 64]).to(device=device, dtype=dtype),\n",
    "            torch.randn([2, 1280]).to(device=device, dtype=dtype),\n",
    "            torch.tensor(ort_output).to(device=device, dtype=dtype),\n",
    "            res_hidden_states, \n",
    "        ),\n",
    "        output_path = Path(f'{save_path}/unet/uup-{3}-{i}_origin.onnx'),\n",
    "        ordered_input_names=[\n",
    "            \"sample\", \"emb\", \"encoder_hidden_states\", \n",
    "            \"res_samples0\"\n",
    "        ],\n",
    "        output_names=[\"out_sample\"],  # has to be different from \"sample\" for correct tracing\n",
    "        dynamic_axes={\n",
    "            \"sample\": {0:\"b\"},        \n",
    "        },\n",
    "        opset=11,\n",
    "        use_external_data_format=True,  # UNet is > 2GB, so the weights need to be split\n",
    "    )\n",
    "    quantize_dynamic(\n",
    "        model_input     =   f'{save_path}/unet/uup-{3}-{i}_origin.onnx',\n",
    "        model_output    =   f'{save_path}/unet/uup-{3}-{i}_quant.onnx',\n",
    "        per_channel     =   False,\n",
    "        reduce_range    =   False,\n",
    "        weight_type     =   QuantType.QUInt8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961M\t../onnx_models_cuda/unet/udown_origin.onnx\n",
      "371M\t../onnx_models_cuda/unet/umid_origin.onnx\n",
      "4.0K\t../onnx_models_cuda/unet/upost_origin.onnx\n",
      "4.0K\t../onnx_models_cuda/unet/upre_origin.onnx\n",
      "619M\t../onnx_models_cuda/unet/uup-0_origin.onnx\n",
      "1002M\t../onnx_models_cuda/unet/uup-1_origin.onnx\n",
      "479M\t../onnx_models_cuda/unet/uup-2_origin.onnx\n",
      "1.1G\t../onnx_models_cuda/unet/uup-3-0_origin.onnx\n",
      "1.1G\t../onnx_models_cuda/unet/uup-3-1_origin.onnx\n",
      "1.1G\t../onnx_models_cuda/unet/uup-3-2_origin.onnx\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../onnx_models_cuda/unet/**_origin.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242M\t../onnx_models_cuda/unet/udown_quant.onnx\n",
      "94M\t../onnx_models_cuda/unet/umid_quant.onnx\n",
      "4.0K\t../onnx_models_cuda/unet/upost_quant.onnx\n",
      "4.0K\t../onnx_models_cuda/unet/upre_quant.onnx\n",
      "155M\t../onnx_models_cuda/unet/uup-0_quant.onnx\n",
      "263M\t../onnx_models_cuda/unet/uup-1_quant.onnx\n",
      "275M\t../onnx_models_cuda/unet/uup-2_quant.onnx\n",
      "1.1G\t../onnx_models_cuda/unet/uup-3-0_quant.onnx\n",
      "1.1G\t../onnx_models_cuda/unet/uup-3-1_quant.onnx\n",
      "1.1G\t../onnx_models_cuda/unet/uup-3-2_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "!du -sh ../onnx_models_cuda/unet/**_quant.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ðŸ’š ONNX-Runtime Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = f'{save_path}/unet/upre_quant.onnx'\n",
    "session_pre = ort.InferenceSession(onnx_model_path, providers=['AzureExecutionProvider'])\n",
    "session_down = ort.InferenceSession(onnx_model_path, providers=['AzureExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test running\n",
    "ort_inputs  = {}\n",
    "latents = session_pre.run(None, {\n",
    "    'input': torch.randn([1, 4, 64, 64]).to(dtype = dtype).numpy()\n",
    "})[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
