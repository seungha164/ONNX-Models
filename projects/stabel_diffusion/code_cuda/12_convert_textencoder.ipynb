{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.schedulers import PNDMScheduler\n",
    "from pathlib import Path\n",
    "from diffusers import DiffusionPipeline\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "from onnxruntime.quantization import QuantType\n",
    "\n",
    "from util_onnx import onnx_export\n",
    "import utils\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc14d355a24493aecce59f24bc8c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "save_path = f'../onnx_models_{device}'\n",
    "os.makedirs(save_path, exist_ok = True)\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 0. INPUT 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49407,   320,  3490,  2368, 49406,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = f'{save_path}/tokenizer/to_quant.onnx'\n",
    "sessTokenizer = ort.InferenceSession(onnx_model_path, providers=['AzureExecutionProvider'])\n",
    "\n",
    "ascii_str   = utils.toAsciiTensor()\n",
    "text_ids = sessTokenizer.run(None, {\n",
    "    'input' : ascii_str.detach().cpu().numpy()\n",
    "})[0]\n",
    "text_ids = torch.tensor(text_ids).to(device = device)\n",
    "text_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "##### 1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncond_input = pipeline.tokenizer(\n",
    "            [\"\"],\n",
    "            padding         = \"max_length\",\n",
    "            max_length      = 77,\n",
    "            truncation      = True,\n",
    "            return_tensors  = \"pt\",\n",
    "        ).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, textencoder, uncond_input, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.text_encoder = textencoder.to(device = device)\n",
    "        self.uncond_input = uncond_input\n",
    "        self.device = device\n",
    "       \n",
    "    def forward(self, text_ids):\n",
    "        textembed       = self.text_encoder(\n",
    "            text_ids.to(device=self.device, dtype=torch.int32)\n",
    "        ).last_hidden_state\n",
    "        negative_prompt_embeds  = self.text_encoder(\n",
    "            self.uncond_input.to(device=self.device, dtype=torch.int32)\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, textembed])\n",
    "        return prompt_embeds\n",
    "TextEmbedding(pipeline.text_encoder, uncond_input)(text_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### 💛 Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:281: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:289: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:321: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "# onnx conversion\n",
    "os.makedirs(f'{save_path}/text_encoder/', exist_ok= True)\n",
    "torch.onnx.export(\n",
    "    model               =   TextEmbedding(pipeline.text_encoder, uncond_input, device=device),                            # 실행될 모델\n",
    "    args                =   (text_ids),        # 모델 입력값(tuple or 여러 입력값)\n",
    "    f                   =   f'{save_path}/text_encoder/te_origin.onnx',                     # 모델 저장 경로\n",
    "    export_params       =   True,                 # 모델 파일 안에 학습된 모델 가중치 저장 여부\n",
    "    opset_version       =   14,                   # 모델 변환할 때 사용할 onnx 버전\n",
    "    do_constant_folding =   True,         # 최적화시 상수폴딩 사용할지 여부\n",
    "    input_names     =   ['input'],\n",
    "    output_names    =   [\"output\"],\n",
    "    dynamic_axes    =   {\n",
    "        'input'     : {0 : 'batch_size'},    # 가변적인 길이를 가진 차원\n",
    "    }\n",
    ") \n",
    "# model quantization\n",
    "quantize_dynamic(\n",
    "    model_input     =   f'./{save_path}/text_encoder/te_origin.onnx', \n",
    "    model_output    =   f'./{save_path}/text_encoder/te_quant.onnx', \n",
    "    per_channel     =   False,\n",
    "    reduce_range    =   False,\n",
    "    weight_type     =   QuantType.QUInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 💚 ONNX-Runtime Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names: ['input']\n",
      "Output names: ['output']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = f'{save_path}/text_encoder/te_quant.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Print the input names and shapes\n",
    "input_names = [input.name for input in session.get_inputs()]\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "print(\"Input names:\", input_names)\n",
    "print(\"Output names:\", output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 77, 768)\n",
      "[[[-0.3885849   0.02323209 -0.05236366 ... -0.48972186 -0.30705208\n",
      "    0.06718016]\n",
      "  [-0.3963155  -1.440579   -0.3369534  ...  0.9637965   0.17683399\n",
      "   -1.0900829 ]\n",
      "  [-0.52471566 -1.461724   -0.30159184 ...  1.0555189   0.0728555\n",
      "   -1.0248568 ]\n",
      "  ...\n",
      "  [ 0.5502783  -0.9023385  -0.5174666  ...  1.6339296  -1.0447981\n",
      "   -0.26709685]\n",
      "  [ 0.5530994  -0.89288574 -0.51752    ...  1.681461   -1.0652006\n",
      "   -0.26443684]\n",
      "  [ 0.5487205  -0.73710895 -0.35334367 ...  1.632203   -1.0066445\n",
      "   -0.2860986 ]]\n",
      "\n",
      " [[-0.367568    0.05707917  0.01100038 ... -0.469685   -0.21827777\n",
      "    0.05954561]\n",
      "  [-0.27619705 -1.1812177  -0.16578478 ...  0.3780036   0.4180037\n",
      "   -1.3891574 ]\n",
      "  [-0.5270904   0.56846243  1.5313094  ...  1.4484036  -0.8622036\n",
      "   -0.73963755]\n",
      "  ...\n",
      "  [ 0.34101295 -0.5812703   0.76695126 ...  0.04324484  0.87072957\n",
      "   -1.2074572 ]\n",
      "  [ 0.7566298  -1.0436481   0.5474509  ...  0.12419611  0.77519834\n",
      "   -1.2423668 ]\n",
      "  [-0.14187689 -0.872546    1.1237452  ... -0.5237114   0.9084585\n",
      "   -0.6733153 ]]]\n"
     ]
    }
   ],
   "source": [
    "# test running\n",
    "ort_inputs  = {'input': text_ids.detach().cpu().numpy()}\n",
    "ort_outputs = session.run(None, ort_inputs)\n",
    "print(ort_outputs[0].shape)\n",
    "print(ort_outputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
